{"id": "bdd24bc5-127b-4933-9737-ecb0645eb52d", "text": "tional to the negative of the gradient of the function at the current point.\nRecall from Section 5.1 that the gradient points in the direction of theWe use the\nconvention of row\nvectors for\ngradients.\nsteepest ascent. Another useful intuition is to consider the set of lines\nwhere the function is at a certain value (f(x) = cfor some value câˆˆR),\nwhich are known as the contour lines. The gradient points in a direction\nthat is orthogonal to the contour lines of the function we wish to optimize.", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 233, "page_label": "228"}}