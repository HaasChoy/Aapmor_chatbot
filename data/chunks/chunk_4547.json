{"id": "d2ce4857-995d-4d51-9001-194ba14353b0", "text": "computing the gradient of L, setting it to 0 and solving for θ.\nUsing the results from Chapter 5, we compute the gradient of Lwith\nrespect to the parameters as\ndL\ndθ = d\ndθ\n( 1\n2σ2 (y−Xθ)⊤(y−Xθ)\n)\n(9.11a)\n= 1\n2σ2\nd\ndθ\n(\ny⊤y−2y⊤Xθ+ θ⊤X⊤Xθ\n)\n(9.11b)\n= 1\nσ2 (−y⊤X+ θ⊤X⊤X) ∈R1×D. (9.11c)\nThe maximum likelihood estimator θML solves dL\ndθ = 0⊤ (necessary opti-\nmality condition) and we obtainIgnoring the\npossibility of\nduplicate data\npoints, rk(X) = D\nif N ⩾D, i.e., we\ndo not have more\nparameters than", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 299, "page_label": "294"}}