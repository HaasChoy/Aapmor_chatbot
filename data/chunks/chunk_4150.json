{"id": "6c2d2631-45ef-4468-9476-6a784cbffda8", "text": "if we use the Euclidean norm. The gradient of (7.9) with respect to xis\n∇x = 2(Ax−b)⊤A. (7.10)\nWe can use this gradient directly in a gradient descent algorithm. How-\never, for this particular special case, it turns out that there is an analytic\nsolution, which can be found by setting the gradient to zero. We will see\nmore on solving squared error problems in Chapter 9.\nRemark. When applied to the solution of linear systems of equationsAx=", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 235, "page_label": "230"}}