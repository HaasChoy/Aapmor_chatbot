{"id": "0a9e8c72-1e48-4117-83e1-252e7e6260d7", "text": "partial derivatives to zero. By setting (12.35) to zero, we ﬁnd\nw=\nN∑\nn=1\nαnynxn, (12.38)\nwhich is a particular instance of the representer theorem (Kimeldorf andrepresenter theorem\nWahba, 1970). Equation (12.38) states that the optimal weight vector inThe representer\ntheorem is actually\na collection of\ntheorems saying\nthat the solution of\nminimizing\nempirical risk lies in\nthe subspace\n(Section 2.4.3)\ndeﬁned by the\nexamples.", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 389, "page_label": "384"}}