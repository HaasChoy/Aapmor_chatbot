{"id": "060fc1dc-07c1-40a7-92b1-ddffca55fb36", "text": "9.4.3 Encoder-Decoder Model\nA model speciÔ¨Åcally developed for forecasting variable length output sequences is called the\nEncoder-Decoder LSTM. The model was designed for prediction problems where there are\nboth input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such\nas translating text from one language to another. This model can be used for multi-step time\nseries forecasting. As its name suggests, the model is comprised of two sub-models: the encoder", "metadata": {"producer": "pdfTeX-1.40.18", "creator": "LaTeX with hyperref package", "creationdate": "2018-11-01T07:53:25+11:00", "author": "", "title": "", "subject": "", "keywords": "", "moddate": "2018-11-01T07:53:25+11:00", "trapped": "/False", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Deep Learning for Time Series Forecasting - Predict the Future with MLPs, CNNs and LSTMs in Python (Jason Brownlee) (Z-Library).pdf", "total_pages": 574, "page": 164, "page_label": "148"}}