{"id": "e2cd6e05-c8c2-4659-ab70-da568e18a969", "text": "know what was predicted for the prior day in the sequence and accumulate internal state while\noutputting the sequence. Let’s take a closer look at how this model is deﬁned.\nAs before, we deﬁne an LSTM hidden layer with 200 units. This is the decoder model that\nwill read the input sequence and will output a 200 element vector (one output per unit) that\ncaptures features from the input sequence. We will use 14 days of total power consumption as\ninput.\n# define encoder", "metadata": {"producer": "pdfTeX-1.40.18", "creator": "LaTeX with hyperref package", "creationdate": "2018-11-01T07:53:25+11:00", "author": "", "title": "", "subject": "", "keywords": "", "moddate": "2018-11-01T07:53:25+11:00", "trapped": "/False", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Deep Learning for Time Series Forecasting - Predict the Future with MLPs, CNNs and LSTMs in Python (Jason Brownlee) (Z-Library).pdf", "total_pages": 574, "page": 421, "page_label": "405"}}