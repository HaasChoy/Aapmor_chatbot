{"id": "04e6e225-afb2-4170-ba08-db810fc59016", "text": "latent-variable models (at least via maximum likelihood) can be done in a\nprincipled way using the expectation maximization (EM) algorithm (Demp-\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\nare helpful, are principal component analysis for dimensionality reduc-\ntion (Chapter 10), Gaussian mixture models for density estimation (Chap-\nter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\n(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 280, "page_label": "275"}}