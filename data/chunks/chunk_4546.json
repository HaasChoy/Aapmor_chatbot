{"id": "55e9add1-5d9c-4205-902f-7fdb33b71771", "text": "Recall from\nSection 3.1 that\n∥x∥2 = x⊤xif we\nchoose the dot\nproduct as the inner\nproduct.\nWith (9.10b), we have now a concrete form of the negative log-likelihood\nfunction we need to optimize. We immediately see that (9.10b) is quadratic\nin θ. This means that we can ﬁnd a unique global solution θML for mini-\nmizing the negative log-likelihood L. We can ﬁnd the global optimum by\ncomputing the gradient of L, setting it to 0 and solving for θ.", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 299, "page_label": "294"}}