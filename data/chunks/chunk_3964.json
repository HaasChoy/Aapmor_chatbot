{"id": "066e8a5d-acaa-461e-a91a-40f6d09e26a5", "text": "the statistical manifold. Just like the Euclidean distance is a special case of\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\ntwo more general classes of divergences called Bregman divergences and\nf-divergences. The study of divergences is beyond the scope of this book,\nand we refer for more details to the recent book by Amari (2016), one of\nthe founders of the ﬁeld of information geometry . ♦\n6.5 Gaussian Distribution", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 202, "page_label": "197"}}