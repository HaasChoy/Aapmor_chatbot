{"id": "f069a531-a761-4454-abc1-4f9606e9b0a4", "text": "function value f(x).\nExample 5.1\nRecall the dot product as a special case of an inner product (Section 3.2).\nIn the previous notation, the function f(x) = x⊤x, x ∈R2, would be\nspeciﬁed as\nf : R2 →R (5.2a)\nx↦→x2\n1 + x2\n2 . (5.2b)\nIn this chapter, we will discuss how to compute gradients of functions,\nwhich is often essential to facilitate learning in machine learning models\nsince the gradient points in the direction of steepest ascent. Therefore,", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 145, "page_label": "140"}}