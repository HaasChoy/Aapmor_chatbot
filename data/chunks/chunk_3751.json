{"id": "a27be035-81dc-4e5e-a39c-fcb3cfd2b490", "text": "5.6 Backpropagation and Automatic Differentiation 161\nFigure 5.9\nBackward pass in a\nmulti-layer neural\nnetwork to compute\nthe gradients of the\nloss function.\nx fK\nA0,b0 AK−1,bK−1\nLfK−1\nAK−2,bK−2\nf1\nA1,b1\nFigure 5.10 Simple\ngraph illustrating\nthe ﬂow of data\nfrom xto yvia\nsome intermediate\nvariables a,b.\nx a b y\nneed to compute are indicated by the boxes. Figure 5.9 visualizes that the\ngradients are passed backward through the network.\n5.6.2 Automatic Differentiation", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 166, "page_label": "161"}}