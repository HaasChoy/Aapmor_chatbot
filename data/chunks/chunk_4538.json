{"id": "5d6406c4-926d-4a2c-83a9-ff05e7acb0e3", "text": "it is unnormalized), and may not even be integrable with respect to θ.\nHowever, the likelihood in (9.7) is a normalized probability distribution\nin y. ♦\nTo ﬁnd the desired parameters θML that maximize the likelihood, we\ntypically perform gradient ascent (or gradient descent on the negative\nlikelihood). In the case of linear regression we consider here, however, Since the logarithm\nis a (strictly)\nmonotonically\nincreasing function,\nthe optimum of a\nfunction f is\nidentical to the", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 298, "page_label": "293"}}