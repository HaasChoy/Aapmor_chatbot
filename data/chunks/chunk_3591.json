{"id": "a0f3be61-dfe2-4f44-96a2-61b79b0f38e3", "text": "4.6 Matrix Approximation 129\ngular matrices. Then, for A∈Rm×n and m⩾n,\nA\nm×n\n= U\nm×n\nΣ\nn×n\nV⊤\nn×n\n. (4.89)\nSometimes this formulation is called thereduced SVD (e.g., Datta (2010)) reduced SVD\nor the SVD (e.g., Press et al. (2007)). This alternative format changes\nmerely how the matrices are constructed but leaves the mathematical\nstructure of the SVD unchanged. The convenience of this alternative\nformulation is that Σ is diagonal, as in the eigenvalue decomposition.", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 134, "page_label": "129"}}