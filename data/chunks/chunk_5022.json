{"id": "67c4558b-025e-4dad-bcd2-ae1f85de53f7", "text": "Shalev-Shwartz and Ben-David, 2014). ♦\n12.2.2 Traditional Derivation of the Margin\nIn the previous section, we derived (12.10) by making the observation that\nwe are only interested in the direction of wand not its length, leading to\nthe assumption that ∥w∥= 1. In this section, we derive the margin max-\nimization problem by making a different assumption. Instead of choosing\nthat the parameter vector is normalized, we choose a scale for the data.", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 381, "page_label": "376"}}