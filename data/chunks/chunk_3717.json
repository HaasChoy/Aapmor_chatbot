{"id": "b976d992-97fd-48f2-950f-44292c430284", "text": "We collect the partial derivatives in the Jacobian and obtain the gradient\ndf\ndx =\n\n\n∂f1\n∂x1\n··· ∂f1\n∂xN\n... ...\n∂fM\n∂x1\n··· ∂fM\n∂xN\n\n=\n\n\nA11 ··· A1N\n... ...\nAM1 ··· AMN\n\n= A∈RM×N . (5.68)\nExample 5.10 (Chain Rule)\nConsider the function h: R →R, h(t) = (f ◦g)(t) with\nf : R2 →R (5.69)\ng: R →R2 (5.70)\nf(x) = exp(x1x2\n2) , (5.71)\nx=\n[x1\nx2\n]\n= g(t) =\n[tcos t\ntsin t\n]\n(5.72)\nand compute the gradient of hwith respect to t. Since f : R2 →R and\ng: R →R2 we note that\n∂f\n∂x ∈R1×2 , ∂g", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 158, "page_label": "153"}}