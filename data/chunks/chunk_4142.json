{"id": "812e6ef4-88e0-4514-9a86-6f7ebc077f9d", "text": "out.\nThis observation allows us to deﬁne a simple gradient descent algo-\nrithm: If we want to ﬁnd a local optimum f(x∗) of a function f : Rn →\nR, x↦→f(x), we start with an initial guessx0 of the parameters we wish\nto optimize and then iterate according to\nxi+1 = xi −γi((∇f)(xi))⊤. (7.6)\nFor suitable step-size γi, the sequence f(x0) ⩾f(x1) ⩾... converges to\na local minimum.\nExample 7.1\nConsider a quadratic function in two dimensions\nf\n([x1\nx2\n])\n= 1\n2\n[x1\nx2\n]⊤[2 1\n1 20\n][x1\nx2\n]\n−\n[5\n3\n]⊤[x1\nx2", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 233, "page_label": "228"}}