{"id": "388f1025-9a81-4d8f-86f9-d1833813112d", "text": "the identity matrix.\nence between the original data xn and their projections ˜xn, is equivalent\nto ﬁnding the best rank- M approximation BB⊤of the identity matrix I\n(see Section 4.6). ♦\nNow we have all the tools to reformulate the loss function (10.29).\nJM = 1\nN\nN∑\nn=1\n∥xn −˜xn∥2 (10.38b)\n= 1\nN\nN∑\nn=1\n\nD∑\nj=M+1\n(b⊤\nj xn)bj\n\n2\n. (10.41)\nWe now explicitly compute the squared norm and exploit the fact that the\nbj form an ONB, which yields\nJM = 1\nN\nN∑\nn=1\nD∑\nj=M+1\n(b⊤\nj xn)2 = 1\nN\nN∑\nn=1", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 336, "page_label": "331"}}