{"id": "13efe6b2-6e97-49ce-93b8-58bc2a6d9645", "text": "x2\n(f) Undo the standardization\nand move projected data back\ninto the original data space\nfrom (a).\nresponding eigenvalue. The longer vector spans the principal subspace,\nwhich we denote by U. The data covariance matrix is represented by\nthe ellipse.\n4. Projection We can project any data pointx∗∈RD onto the principal\nsubspace: To get this right, we need to standardize x∗using the mean\nµd and standard deviation σd of the training data in thedth dimension,\nrespectively , so that\nx(d)\n∗ ←x(d)", "metadata": {"producer": "pdfTeX-1.40.20", "creator": "LaTeX with hyperref", "creationdate": "2019-10-15T12:48:02+01:00", "author": "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong", "keywords": "", "moddate": "2020-01-03T10:00:34+02:00", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1", "subject": "", "title": "Mathematics for Machine Learning", "trapped": "/False", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Mathematics for Machine Learning ( etc.) (Z-Library).pdf", "total_pages": 417, "page": 342, "page_label": "337"}}