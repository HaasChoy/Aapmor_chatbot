{"id": "46c0723f-f7d0-4f59-8478-953722eb4847", "text": "14.4. Multilayer Perceptron Model 258\ndefault) on the output layer because we are predicting a continuous value. The loss function for\nthe network will be the mean squared error loss, or MSE, and we will use the eﬃcient Adam\nﬂavor of stochastic gradient descent to train the network.\n# define model\nmodel = Sequential()\nmodel.add(Dense(n_nodes, activation=' relu' , input_dim=n_input))\nmodel.add(Dense(1))\nmodel.compile(loss=' mse' , optimizer=' adam' )", "metadata": {"producer": "pdfTeX-1.40.18", "creator": "LaTeX with hyperref package", "creationdate": "2018-11-01T07:53:25+11:00", "author": "", "title": "", "subject": "", "keywords": "", "moddate": "2018-11-01T07:53:25+11:00", "trapped": "/False", "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3", "source": "/home/haas/rag_proj/samples/AAPMOR Website/Deep Learning for Time Series Forecasting - Predict the Future with MLPs, CNNs and LSTMs in Python (Jason Brownlee) (Z-Library).pdf", "total_pages": 574, "page": 274, "page_label": "258"}}